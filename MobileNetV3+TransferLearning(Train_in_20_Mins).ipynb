{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "MobileNetV3+TransferLearning(Train in 20 Mins)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrSaggio/mrSaggio/blob/main/MobileNetV3%2BTransferLearning(Train_in_20_Mins).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'chest-xray-pneumonia:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F17810%2F23812%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240604%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240604T101056Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3ccd5fac6d68d76c80db58cb5641b3388ffd792921a7e9c9b9810f75775f52b64ccac195141b76c7f78e59eb48e9a2eae1245670fa0e10a7fa0321a390cdd2d51e798eb30f78b130b1757a2476dc41fc0538c393a6ac3d86544f49c5f551c8f830afb6ffce65115eda9898ccc11f9ac1344bfda631ad56bc6bd8138c97b67615f3327e4456563157be683d690a266ad91952b030b1154b1855bef6f225c3bf74060bad1f7b15dcdbb3eb12cb89caab39d8e04f5e9e409c6855e8a0b77c7ff6f844665af4e9e908f51f59f476c8bc2523c619b40169689d7f97f5f03d008c42e0caa45971a3c9911fdcdd4e7caa2d4ba632f8e9b7e2693c6e38ade0f61812b203'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "X0pb5zoruwjZ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we use a CNN to classify pneumonia cases. The dataset in question is available at kaggle by [this link](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia). The problem has been solved by the use of ShuffleNet and MobileNet's previous versions before, but in this kernel we are using a small MobileNetV3 via transfer learning and then finetuning its last layers in addition to learn a new classification head. This makes this solution extremly lightweight. Stay Tuned!"
      ],
      "metadata": {
        "id": "FFWO57zZRHkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "3_9GX6gLRFKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.9.0 -q\n",
        "!pip install torchvision==0.10.0 -q\n",
        "!pip install --upgrade scikit-learn -q"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-17T05:49:28.914682Z",
          "iopub.execute_input": "2021-08-17T05:49:28.915112Z",
          "iopub.status.idle": "2021-08-17T05:51:10.923174Z",
          "shell.execute_reply.started": "2021-08-17T05:49:28.915025Z",
          "shell.execute_reply": "2021-08-17T05:51:10.922113Z"
        },
        "trusted": true,
        "id": "kFqSELdcuwje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "RnKm58M_b5vF",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:10.927193Z",
          "iopub.execute_input": "2021-08-17T05:51:10.9275Z",
          "iopub.status.idle": "2021-08-17T05:51:11.672041Z",
          "shell.execute_reply.started": "2021-08-17T05:51:10.927469Z",
          "shell.execute_reply": "2021-08-17T05:51:11.671135Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "1x2vG-SZQVRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import h5py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (f1_score,\n",
        "                             classification_report,\n",
        "                             plot_confusion_matrix,\n",
        "                             ConfusionMatrixDisplay)\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
        "from torchvision import transforms as VisTransforms\n",
        "import torchvision\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import multiprocessing as mp\n",
        "\n",
        "\n",
        "plt.style.use('ggplot')"
      ],
      "metadata": {
        "id": "cNLwxmlcjbos",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:11.674388Z",
          "iopub.execute_input": "2021-08-17T05:51:11.674842Z",
          "iopub.status.idle": "2021-08-17T05:51:13.64465Z",
          "shell.execute_reply.started": "2021-08-17T05:51:11.674799Z",
          "shell.execute_reply": "2021-08-17T05:51:13.64392Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apparently there is a problem with pytorch 1.9's implementation of torch hub which this helps to alieviate."
      ],
      "metadata": {
        "id": "53hth5vjQnFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.hub._validate_not_a_forked_repo=lambda a,b,c: True"
      ],
      "metadata": {
        "id": "JiXX3HU_SC3r",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:13.646156Z",
          "iopub.execute_input": "2021-08-17T05:51:13.646645Z",
          "iopub.status.idle": "2021-08-17T05:51:13.650249Z",
          "shell.execute_reply.started": "2021-08-17T05:51:13.646614Z",
          "shell.execute_reply": "2021-08-17T05:51:13.649615Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Constants"
      ],
      "metadata": {
        "id": "okZ0P94uQwgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "DATASET_PATH = \"../input/chest-xray-pneumonia/chest_xray\"\n",
        "SAVE_LOCATION = \"output/\"\n",
        "Path(SAVE_LOCATION).mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_SIZE = (256, 256)\n",
        "\n",
        "LR = 1e-2\n",
        "PRETRAINED_LR = 1e-6\n",
        "MOMENTUM = 0.9\n",
        "BETAS = (0.9, 0.999)\n",
        "ADAM_EPS = 1e-8\n",
        "WEIGHT_DECAY = 1e-1\n",
        "MAX_GRADIENT_NORM = 0.5\n",
        "\n",
        "LR_SCHEDULE_GAMMA = 0.9\n",
        "T_0_COSINE_ANNEALING = 10\n",
        "T_MULT_COSINE_ANNEALING = 2\n",
        "\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = 20\n",
        "\n",
        "BATCH_SIZE = {\n",
        "    \"train\": 128,\n",
        "    \"test\": 256,\n",
        "}\n",
        "\n",
        "MAX_EPOCHS = 100"
      ],
      "metadata": {
        "id": "DizLlq5DMn-6",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:13.651318Z",
          "iopub.execute_input": "2021-08-17T05:51:13.652777Z",
          "iopub.status.idle": "2021-08-17T05:51:13.667247Z",
          "shell.execute_reply.started": "2021-08-17T05:51:13.652699Z",
          "shell.execute_reply": "2021-08-17T05:51:13.666281Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-training plots\n",
        "\n",
        "### Plotting widths and heights of the images in the dataset\n",
        "\n",
        "\n",
        "This helps us choose a better size (I've chosen it before;).) The chosen input dimension is 256x256, because:\n",
        "\n",
        "1 - All image sizes are bigger than this.\n",
        "\n",
        "2 - Bigger image size means more detials, and we can afford to use bigger image sizes because we're effectively single channel network (although we map grayscale to RGB finally)."
      ],
      "metadata": {
        "id": "fJvy5inTQy3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = []\n",
        "labels = []\n",
        "\n",
        "for root, _, files in os.walk(os.path.join(DATASET_PATH, \"train\")):\n",
        "    for f in files:\n",
        "        if f.endswith(\".jpeg\"):\n",
        "            img = Image.open(os.path.join(root, f))\n",
        "            w, h = img.size\n",
        "            aspect = w / h\n",
        "            sizes.append((w, h, aspect))\n",
        "            labels.append(Path(root).stem)\n",
        "\n",
        "\n",
        "sizes = np.asarray(sizes)\n",
        "\n",
        "w = sizes[:, 0]\n",
        "h = sizes[:, 1]\n",
        "a = sizes[:, 2]\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(8,10))\n",
        "\n",
        "axes[0].hist(w, bins=100)\n",
        "axes[0].set_title(\"Width Distribution\")\n",
        "axes[1].hist(h, bins=100)\n",
        "axes[1].set_title(\"Height Distribution\")\n",
        "axes[2].hist(a, bins=20)\n",
        "axes[2].set_title(\"Aspect Distribution\")\n",
        "pass"
      ],
      "metadata": {
        "id": "xT8MFli2m2nX",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:13.668586Z",
          "iopub.execute_input": "2021-08-17T05:51:13.668977Z",
          "iopub.status.idle": "2021-08-17T05:51:47.758607Z",
          "shell.execute_reply.started": "2021-08-17T05:51:13.66892Z",
          "shell.execute_reply": "2021-08-17T05:51:47.757971Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the distribution of classes and choosing adequate weightings."
      ],
      "metadata": {
        "id": "S9pcoB17Sp3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uniques, counts = np.unique(labels, return_counts=True)\n",
        "plt.bar(uniques, counts, color=[\"brown\", \"green\"])\n",
        "plt.title('Distribution of Labels in the Training Set')\n",
        "pass"
      ],
      "metadata": {
        "id": "u1yyutMoTt0g",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:47.75964Z",
          "iopub.execute_input": "2021-08-17T05:51:47.760061Z",
          "iopub.status.idle": "2021-08-17T05:51:47.908922Z",
          "shell.execute_reply.started": "2021-08-17T05:51:47.760027Z",
          "shell.execute_reply": "2021-08-17T05:51:47.908152Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(labels)\n",
        "n_classes = len(set(labels)) # == 2\n",
        "# balanced weight formula: n_total/(n_classes * n_samples_in_ith_class)\n",
        "classification_weights = {\n",
        "    idx: n/(v*n_classes) for idx, (k, v) in enumerate(zip(*np.unique(labels, return_counts=True)))\n",
        "}\n",
        "print(classification_weights)"
      ],
      "metadata": {
        "id": "QoSHBz8nUfg7",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:47.911088Z",
          "iopub.execute_input": "2021-08-17T05:51:47.911534Z",
          "iopub.status.idle": "2021-08-17T05:51:48.049435Z",
          "shell.execute_reply.started": "2021-08-17T05:51:47.911488Z",
          "shell.execute_reply": "2021-08-17T05:51:48.047529Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definitions\n",
        "\n",
        "## Defining Transforms\n",
        "\n",
        "\n",
        "These are some useful transforms that will help us get the data ready to be an input to the CNN."
      ],
      "metadata": {
        "id": "AL2IrEKaRCNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelTransformer:\n",
        "    def __call__(self, label):\n",
        "        label = label.lower()\n",
        "        if label == \"normal\":\n",
        "            return 0\n",
        "        else:\n",
        "            return 1"
      ],
      "metadata": {
        "id": "Xo9qx_QZL0NG",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.051427Z",
          "iopub.execute_input": "2021-08-17T05:51:48.051767Z",
          "iopub.status.idle": "2021-08-17T05:51:48.058787Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.051699Z",
          "shell.execute_reply": "2021-08-17T05:51:48.057988Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageResizer:\n",
        "    def __init__(self, size, optional=False):\n",
        "        self.size = size\n",
        "        self.optional = optional\n",
        "    def __call__(self, input_img):\n",
        "        h, w = input_img.shape\n",
        "        target = cv2.resize(input_img,\n",
        "                            (self.size, self.size),\n",
        "                            interpolation = cv2.INTER_CUBIC)\n",
        "        return target"
      ],
      "metadata": {
        "id": "jp6gbVJJ3Upb",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.060325Z",
          "iopub.execute_input": "2021-08-17T05:51:48.060729Z",
          "iopub.status.idle": "2021-08-17T05:51:48.071882Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.060684Z",
          "shell.execute_reply": "2021-08-17T05:51:48.070967Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProcessor:\n",
        "    def __init__(self,\n",
        "                 pipeline):\n",
        "        self.preprcoessing_pipeline = pipeline\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        processed = self.preprcoessing_pipeline(input_img)\n",
        "        return processed"
      ],
      "metadata": {
        "id": "Annz6ipKqT5E",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.07328Z",
          "iopub.execute_input": "2021-08-17T05:51:48.073681Z",
          "iopub.status.idle": "2021-08-17T05:51:48.084253Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.073644Z",
          "shell.execute_reply": "2021-08-17T05:51:48.083321Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Dataset"
      ],
      "metadata": {
        "id": "bExpvdekXB2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PneumoniaDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 subset,\n",
        "                 preprocessor,\n",
        "                 image_transforms,\n",
        "                 label_transforms,\n",
        "                 root=DATASET_PATH,\n",
        "                 debug=False):\n",
        "        super().__init__()\n",
        "        assert subset in [\"train\", \"test\", \"val\"]\n",
        "        self.folder_path = os.path.join(root, subset)\n",
        "        self.dataset_name = subset\n",
        "\n",
        "        self.preprocessor = preprocessor\n",
        "        self.image_transforms = image_transforms\n",
        "        self.label_transforms = label_transforms\n",
        "\n",
        "        self._preprocess(debug)\n",
        "\n",
        "    def _preprocess(self, debug):\n",
        "        '''\n",
        "            This function saves the processed data into a list\n",
        "\n",
        "            debug: flag to create a debug set using a limited number of samples\n",
        "        '''\n",
        "        images_path = []\n",
        "        labels = []\n",
        "        for root, __, files in os.walk(self.folder_path):\n",
        "            for f in files:\n",
        "                if f.endswith(\".jpeg\"):\n",
        "                    fpath = os.path.join(root, f)\n",
        "                    l = Path(root).stem\n",
        "                    images_path.append(fpath)\n",
        "                    labels.append(self.label_transforms(l))\n",
        "\n",
        "\n",
        "        if debug:\n",
        "            images_path = images_path[:50]\n",
        "            labels = labels[:50]\n",
        "\n",
        "        if os.path.exists(self.dataset_name):\n",
        "            os.remove(self.dataset_name)\n",
        "\n",
        "        self.data = []\n",
        "\n",
        "        for idx, fpath in enumerate(tqdm(images_path,\n",
        "                                         desc=f\"creating {self.dataset_name} set\")):\n",
        "            # reading images as grayscales\n",
        "            img = cv2.imread(fpath, 0)\n",
        "            img = self.preprocessor(img)\n",
        "\n",
        "            self.data.append(\n",
        "                {\n",
        "                    \"image\": img,\n",
        "                    \"label\": labels[idx]\n",
        "                }\n",
        "            )\n",
        "\n",
        "        self._len = len(self.data)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx][\"image\"]\n",
        "        l = self.data[idx][\"label\"]\n",
        "\n",
        "        img = self.image_transforms(img)\n",
        "        l = torch.LongTensor([l])\n",
        "\n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"label\": l\n",
        "        }"
      ],
      "metadata": {
        "id": "btK46my0-4rl",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.08535Z",
          "iopub.execute_input": "2021-08-17T05:51:48.085732Z",
          "iopub.status.idle": "2021-08-17T05:51:48.098895Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.085696Z",
          "shell.execute_reply": "2021-08-17T05:51:48.098238Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Initializer"
      ],
      "metadata": {
        "id": "2uKWVfntXFSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used multiple augmentations in order to further augment the dataset and avoid overfitting. ColorJitter changes the brightness and contrast, using affine transformations we change the rotation, shear, and translation of pixel values and finally using horizontal flip we make sure the model is able to find the pneumonia regions in either sides of the chest."
      ],
      "metadata": {
        "id": "SbOcQc-Juwjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to make sure the data is batched correctly\n",
        "class BatchCollater:\n",
        "    def __call__(self, data):\n",
        "        batch = {k: [] for k in data[-1]}\n",
        "        for d in data:\n",
        "            for k in d:\n",
        "                batch[k].append(d[k])\n",
        "        for k in batch:\n",
        "            batch[k] = torch.stack(batch[k], dim=0)\n",
        "        batch[\"label\"] = batch[\"label\"].squeeze()\n",
        "        return batch\n",
        "\n",
        "class Initializer:\n",
        "    def __init__(self):\n",
        "        '''\n",
        "            This class helps us initiate all the required data stuff with ease.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "\n",
        "    def initialize(self, debug=False):\n",
        "        preprocessor = PreProcessor(\n",
        "            pipeline=VisTransforms.Compose([ImageResizer(IMAGE_SIZE[0]),])\n",
        "            )\n",
        "        image_transforms = VisTransforms.Compose([VisTransforms.ToTensor(),\n",
        "                                                  # GrayScale mapping\n",
        "                                                  VisTransforms.Lambda(\n",
        "                                                      lambda x: torch.cat([x, x, x]).clone()\n",
        "                                                      ),\n",
        "                                                  VisTransforms.RandomChoice(\n",
        "                                                      [VisTransforms.ColorJitter(\n",
        "                                                          brightness=0.3,\n",
        "                                                          contrast=0.3,\n",
        "                                                          ),\n",
        "                                                       VisTransforms.RandomAffine(\n",
        "                                                           degrees=20,\n",
        "                                                           shear=3,\n",
        "                                                           translate=(0.2, 0.2)\n",
        "                                                           ),\n",
        "                                                       VisTransforms.RandomHorizontalFlip(\n",
        "                                                           p=1),\n",
        "                                                       ],),\n",
        "                                                  VisTransforms.Normalize(\n",
        "                                                      mean=[0.485, 0.456, 0.406],\n",
        "                                                      std=[0.229, 0.224, 0.225]\n",
        "                                                      )])\n",
        "        label_transforms = LabelTransformer()\n",
        "\n",
        "        train_dataset = PneumoniaDataset(\"train\",\n",
        "                                         preprocessor,\n",
        "                                         image_transforms,\n",
        "                                         label_transforms,\n",
        "                                         debug=debug)\n",
        "\n",
        "        val_dataset = PneumoniaDataset(\"val\",\n",
        "                                       preprocessor,\n",
        "                                       image_transforms,\n",
        "                                       label_transforms,\n",
        "                                       debug=debug)\n",
        "\n",
        "        test_dataset = PneumoniaDataset(\"test\",\n",
        "                                        preprocessor,\n",
        "                                        image_transforms,\n",
        "                                        label_transforms,\n",
        "                                        debug=debug)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            dataset=train_dataset,\n",
        "            batch_size=BATCH_SIZE[\"train\"],\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            collate_fn=BatchCollater(),\n",
        "            prefetch_factor=2,\n",
        "        )\n",
        "\n",
        "        test_loader_params = val_loader_params = dict(batch_size=BATCH_SIZE[\"test\"],\n",
        "                                                      collate_fn=BatchCollater(),\n",
        "                                                      shuffle=False,)\n",
        "\n",
        "\n",
        "        val_loader = DataLoader(dataset=val_dataset, **val_loader_params)\n",
        "        test_loader = DataLoader(dataset=test_dataset, **test_loader_params)\n",
        "\n",
        "        return dict(\n",
        "            train_dataset=train_dataset,\n",
        "            train_loader=train_loader,\n",
        "            val_dataset=val_dataset,\n",
        "            val_loader=val_loader,\n",
        "            test_dataset=test_dataset,\n",
        "            test_loader=test_loader,\n",
        "        )"
      ],
      "metadata": {
        "id": "qcipVLuJRsF1",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.099885Z",
          "iopub.execute_input": "2021-08-17T05:51:48.100298Z",
          "iopub.status.idle": "2021-08-17T05:51:48.117004Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.100269Z",
          "shell.execute_reply": "2021-08-17T05:51:48.116263Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From https://gist.github.com/jeasinema/ed9236ce743c8efaf30fa2ff732749f5\n",
        "\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def weight_init(m):\n",
        "    '''\n",
        "    Initializes the weights of a module.\n",
        "\n",
        "    Usage:\n",
        "        model = Model()\n",
        "        model.apply(weight_init)\n",
        "    '''\n",
        "    if isinstance(m, nn.Conv1d):\n",
        "        init.normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Conv3d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose1d):\n",
        "        init.normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose3d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.BatchNorm1d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm3d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.LSTMCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.GRU):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.GRUCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)"
      ],
      "metadata": {
        "id": "RkYDZr2zdjzF",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.117984Z",
          "iopub.execute_input": "2021-08-17T05:51:48.118356Z",
          "iopub.status.idle": "2021-08-17T05:51:48.135673Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.118329Z",
          "shell.execute_reply": "2021-08-17T05:51:48.134845Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the PneumoniaNet\n",
        "\n",
        "This CNN uses a small MobileNetV3 architecture, and freezes all but its the last two modules which are then are finetuned via a smaller learning rate than the classification head. The last two modules are An inverted residual module + a 1x1 convolution module. This makes the model extremly lightweight to use and extremly resistant to overfitting because of the lack of parameters."
      ],
      "metadata": {
        "id": "LVQbSLr5WT4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrained():\n",
        "    pretrained_model = torchvision.models.mobilenetv3.mobilenet_v3_small(pretrained=True,\n",
        "                                                                         progress=True)\n",
        "    return pretrained_model.features\n",
        "\n",
        "\n",
        "class PneumoniaNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 finetune=False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.loss_fn = nn.CrossEntropyLoss(\n",
        "            weight=torch.FloatTensor(list(classification_weights.values()))\n",
        "            )\n",
        "\n",
        "        self.input_dim = torch.as_tensor(input_dim)\n",
        "\n",
        "        self.input_encoder = load_pretrained()\n",
        "\n",
        "        # Freezing all layers but the last two\n",
        "        for param in self.input_encoder[:-2].parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.output_decoder = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(576, 2),\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "\n",
        "    def _init_weights(self):\n",
        "        self.output_decoder.apply(weight_init)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[-2] == self.input_dim[0] and x.shape[-1] == self.input_dim[1]\n",
        "\n",
        "        x = self.input_encoder(x)\n",
        "        x = self.output_decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def loss(self, outputs, targets):\n",
        "        return self.loss_fn(outputs, targets)\n",
        "\n",
        "\n",
        "\n",
        "    def generate_opt(self):\n",
        "        params = [\n",
        "                  {\"params\": nn.ModuleList([self.output_decoder]).parameters()},\n",
        "                  {\"params\": self.input_encoder[-2:].parameters(), \"lr\": PRETRAINED_LR,}\n",
        "        ]\n",
        "\n",
        "        return torch.optim.AdamW(\n",
        "                params,\n",
        "                lr=LR,\n",
        "                weight_decay=WEIGHT_DECAY,\n",
        "                betas=BETAS,\n",
        "                eps=ADAM_EPS\n",
        "        )\n"
      ],
      "metadata": {
        "id": "LI75MELzUze7",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.13751Z",
          "iopub.execute_input": "2021-08-17T05:51:48.13805Z",
          "iopub.status.idle": "2021-08-17T05:51:48.154141Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.138008Z",
          "shell.execute_reply": "2021-08-17T05:51:48.152917Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d\n",
        "class EarlyStopping:\n",
        "    def __init__(self, mode='min',\n",
        "                 min_delta=0,\n",
        "                 patience=EARLY_STOPPING_PATIENCE,\n",
        "                 percentage=False):\n",
        "        self.mode = mode\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "        self.best = None\n",
        "        self.num_bad_epochs = 0\n",
        "        self.is_better = None\n",
        "        self._init_is_better(mode, min_delta, percentage)\n",
        "\n",
        "        # This helps us saving the model\n",
        "        # whenever this is true we will save the model\n",
        "        self.testing_patience = False\n",
        "\n",
        "        if patience == 0:\n",
        "            self.is_better = lambda a, b: True\n",
        "            self.step = lambda a: False\n",
        "\n",
        "    def stop(self, metrics):\n",
        "        if self.best is None:\n",
        "            self.best = metrics\n",
        "            return False\n",
        "\n",
        "        if np.isnan(metrics):\n",
        "            self.testing_patience = True\n",
        "            return True\n",
        "\n",
        "        if self.is_better(metrics, self.best):\n",
        "            self.num_bad_epochs = 0\n",
        "            self.best = metrics\n",
        "            self.testing_patience = False\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "            self.testing_patience = True\n",
        "\n",
        "        if self.num_bad_epochs >= self.patience:\n",
        "            self.testing_patience = True\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _init_is_better(self, mode, min_delta, percentage):\n",
        "        if mode not in {'min', 'max'}:\n",
        "            raise ValueError('mode ' + mode + ' is unknown!')\n",
        "        if not percentage:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - min_delta\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + min_delta\n",
        "        else:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - (\n",
        "                            best * min_delta / 100)\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + (\n",
        "                            best * min_delta / 100)"
      ],
      "metadata": {
        "id": "lLqeJH-May_c",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.155808Z",
          "iopub.execute_input": "2021-08-17T05:51:48.156328Z",
          "iopub.status.idle": "2021-08-17T05:51:48.170771Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.156287Z",
          "shell.execute_reply": "2021-08-17T05:51:48.17002Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Trainer Class"
      ],
      "metadata": {
        "id": "SExTAXljVy94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are a bunch of helper functions and classes, just to use in the trainer class. The most intersting point is that we are using a CosineAnnealing learning rate scheme that has warmups every few steps with a dynamic cyclical routine. The cycle lengthens everytime it completes a new one."
      ],
      "metadata": {
        "id": "wDtTWYncVrwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_f1(outputs, targets):\n",
        "    binary_outputs = outputs.argmax(dim=-1).detach().cpu().squeeze().numpy()\n",
        "    targets = targets.detach().cpu().squeeze().numpy()\n",
        "    return f1_score(targets, binary_outputs)\n",
        "\n",
        "# A Simple Helper class to circumvent sklearn's need for classifiers\n",
        "class IdentityClassifier:\n",
        "    def __init__(self):\n",
        "        self._estimator_type = 'classifier'\n",
        "        self._classes = [0, 1]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X\n",
        "\n",
        "def classification_report_string_with_dict(**params):\n",
        "    if \"output_dict\" in params:\n",
        "        params.pop(\"output_dict\")\n",
        "    return classification_report(**params), classification_report(output_dict=True, **params)\n"
      ],
      "metadata": {
        "id": "3dgCDkKkVqkX",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.171842Z",
          "iopub.execute_input": "2021-08-17T05:51:48.172327Z",
          "iopub.status.idle": "2021-08-17T05:51:48.186602Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.172295Z",
          "shell.execute_reply": "2021-08-17T05:51:48.18579Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I mentioned above, we use LR scheduling (cosine annealing with warm restarts) and early stopping regularizations, which chooses the best model by it's validation loss value."
      ],
      "metadata": {
        "id": "a1ljxVy4u3Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 model_name,\n",
        "                 train_loader,\n",
        "                 val_loader,\n",
        "                 test_loader,\n",
        "                 max_epochs=MAX_EPOCHS,\n",
        "                 amp=True,\n",
        "                 progress=False,\n",
        "                 device=device):\n",
        "\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "\n",
        "        self.opt = self.model.generate_opt()\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            self.opt,\n",
        "            T_0_COSINE_ANNEALING,\n",
        "            T_MULT_COSINE_ANNEALING,\n",
        "            verbose = False,\n",
        "        )\n",
        "\n",
        "        self.stopper = EarlyStopping()\n",
        "\n",
        "        self.cur_epoch = 1\n",
        "        self.max_epochs = max_epochs\n",
        "\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.save_loc = os.path.join(SAVE_LOCATION, model_name)\n",
        "\n",
        "\n",
        "        self.training_info = []\n",
        "\n",
        "        self.amp = amp\n",
        "\n",
        "        self.debug = False\n",
        "        self.progress = progress\n",
        "\n",
        "    @property\n",
        "    def debug(self):\n",
        "        return self._debug\n",
        "\n",
        "    @property\n",
        "    def amp(self):\n",
        "        return self._amp\n",
        "\n",
        "    @amp.setter\n",
        "    def amp(self, flag):\n",
        "        self._amp = flag\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=self._amp)\n",
        "\n",
        "\n",
        "    @debug.setter\n",
        "    def debug(self, flag):\n",
        "        self._debug = flag\n",
        "        if flag is False:\n",
        "            if hasattr(self, \"original_values\"):\n",
        "                for item in self.original_values:\n",
        "                    self.__dict__[item] = self.original_values[item]\n",
        "        else:\n",
        "            self.original_values = {\n",
        "                \"train_loader\": self.train_loader,\n",
        "                \"val_loader\": self.val_loader,\n",
        "                \"test_loader\": self.test_loader,\n",
        "                \"max_epochs\": self.max_epochs,\n",
        "            }\n",
        "            self.train_loader = DataLoader(self.train_loader.dataset,\n",
        "                                           batch_size=2,\n",
        "                                           shuffle=False,\n",
        "                                           num_workers=2,\n",
        "                                           collate_fn=BatchCollater(),\n",
        "                                           prefetch_factor=2)\n",
        "\n",
        "            self.val_loader = DataLoader(self.val_loader.dataset,\n",
        "                                           batch_size=4,\n",
        "                                           collate_fn=BatchCollater(),\n",
        "                                           shuffle=False)\n",
        "\n",
        "            self.test_loader = DataLoader(self.test_loader.dataset,\n",
        "                                          batch_size=4,\n",
        "                                          collate_fn=BatchCollater(),\n",
        "                                          shuffle=False)\n",
        "            self.max_epochs = 1\n",
        "\n",
        "\n",
        "    @property\n",
        "    def save_loc(self):\n",
        "        return self._save_loc\n",
        "\n",
        "    @save_loc.setter\n",
        "    def save_loc(self, path):\n",
        "\n",
        "        self.model_save_location = os.path.join(path, f\"saved_model.model\")\n",
        "        self.training_info_location = os.path.join(path, \"training_info.json\")\n",
        "        self.test_info_location = os.path.join(path, \"test_info.json\")\n",
        "\n",
        "        self._save_loc = path\n",
        "        Path(self._save_loc).mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        state_dict = {\n",
        "            \"training_info\": self.training_info,\n",
        "            \"model_state\": self.model.state_dict(),\n",
        "            \"opt_state\": self.opt.state_dict(),\n",
        "            \"scaler_state\": self.scaler.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(state_dict, self.model_save_location)\n",
        "\n",
        "\n",
        "    def save_training_info(self):\n",
        "        with open(self.training_info_location, \"w\") as jfile:\n",
        "            json.dump(self.training_info, jfile)\n",
        "\n",
        "    def load_model(self):\n",
        "        state_dict = torch.load(self.model_save_location)\n",
        "        self.training_info = state_dict[\"training_info\"]\n",
        "        self.model.load_state_dict(state_dict[\"model_state\"])\n",
        "        self.opt.load_state_dict(state_dict[\"opt_state\"])\n",
        "        self.scaler.load_state_dict(state_dict[\"scaler_state\"])\n",
        "\n",
        "\n",
        "    def validate(self):\n",
        "        running_info = {\n",
        "                    \"loss\": 0,\n",
        "                    \"f1\": 0,\n",
        "                    \"numel\": 0,\n",
        "        }\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                for k in batch:\n",
        "                    batch[k] = batch[k].to(self.device)\n",
        "\n",
        "                outputs = self.model(batch[\"image\"])\n",
        "                loss = self.model.loss(outputs, batch[\"label\"])\n",
        "\n",
        "                batch_size = len(batch[\"image\"])\n",
        "                running_info[\"loss\"] += loss.item() * batch_size\n",
        "                running_info[\"f1\"] += compute_f1(outputs, batch[\"label\"]) * batch_size\n",
        "                running_info[\"numel\"] += batch_size\n",
        "\n",
        "        val_info = {\n",
        "            f\"val_loss\": running_info[\"loss\"] / running_info[\"numel\"],\n",
        "            f\"val_f1\": running_info[\"f1\"] / running_info[\"numel\"]\n",
        "        }\n",
        "\n",
        "        return val_info\n",
        "\n",
        "    def test(self):\n",
        "        outputs = []\n",
        "        targets = []\n",
        "        self.model.eval()\n",
        "        loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in self.test_loader:\n",
        "                for k in batch:\n",
        "                    batch[k] = batch[k].to(self.device)\n",
        "\n",
        "                o = self.model(batch[\"image\"])\n",
        "\n",
        "                loss += self.model.loss(o, batch[\"label\"]).item() * len(batch[\"label\"])\n",
        "\n",
        "                outputs.extend(o.argmax(dim=-1).detach().cpu().tolist())\n",
        "                targets.extend(batch[\"label\"].detach().cpu().tolist())\n",
        "\n",
        "        outputs = np.asarray(outputs).astype(np.int)\n",
        "        targets = np.asarray(targets).astype(np.int)\n",
        "\n",
        "\n",
        "        report, test_info = classification_report_string_with_dict(\n",
        "                y_true=targets,\n",
        "                y_pred=outputs,\n",
        "                target_names = [\"Normal\", \"Pneumonia\"],\n",
        "            )\n",
        "\n",
        "\n",
        "        test_info[\"test_loss\"] = loss / len(outputs)\n",
        "\n",
        "        print(report)\n",
        "        print(f\"test_loss: {test_info['test_loss']}\")\n",
        "\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.grid(None)\n",
        "        plot_confusion_matrix(IdentityClassifier(), outputs, targets,\n",
        "                              display_labels=[\"Normal\", \"Pneumonia\"],\n",
        "                              normalize='true',\n",
        "                              values_format='.2%',\n",
        "                              ax=ax)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        with open(self.test_info_location, \"w\") as jfile:\n",
        "            json.dump(test_info, jfile)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        begin = True\n",
        "        for epoch in range(self.cur_epoch, self.max_epochs+1):\n",
        "            running_info = {\n",
        "                    \"loss\": 0,\n",
        "                    \"f1\": 0,\n",
        "                    \"numel\": 0,\n",
        "                }\n",
        "            if self.progress:\n",
        "                pbar = tqdm(total=len(self.train_loader),\n",
        "                            desc= f\"Epoch {epoch} out of {self.max_epochs}.\",\n",
        "                            leave=False)\n",
        "\n",
        "            for batch in self.train_loader:\n",
        "                for k in batch:\n",
        "                    batch[k] = batch[k].to(self.device)\n",
        "\n",
        "                self.opt.zero_grad()\n",
        "                with torch.cuda.amp.autocast(enabled=self.amp):\n",
        "                    outputs = self.model(batch[\"image\"])\n",
        "                    loss = self.model.loss(outputs, batch[\"label\"])\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.unscale_(self.opt)\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), MAX_GRADIENT_NORM)\n",
        "                self.scaler.step(self.opt)\n",
        "                self.scaler.update()\n",
        "\n",
        "                batch_size = len(batch[\"image\"])\n",
        "                batch_loss = loss.item()\n",
        "                running_info[\"loss\"] += batch_size * batch_loss\n",
        "                running_info[\"f1\"] += compute_f1(outputs, batch[\"label\"]) * batch_size\n",
        "                running_info[\"numel\"] += batch_size\n",
        "\n",
        "                if self.progress:\n",
        "                    pbar.set_postfix(batch_loss=batch_loss)\n",
        "                    pbar.update()\n",
        "                    time.sleep(0.01)\n",
        "\n",
        "            self.cur_epoch = epoch\n",
        "            self.scheduler.step()\n",
        "\n",
        "            if self.progress:\n",
        "                pbar.close()\n",
        "                time.sleep(0.01)\n",
        "\n",
        "            info = {\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": running_info[\"loss\"] / running_info[\"numel\"],\n",
        "                \"train_f1\": running_info[\"f1\"] / running_info[\"numel\"],\n",
        "            }\n",
        "\n",
        "            info.update(self.validate())\n",
        "            self.training_info.append(info)\n",
        "\n",
        "            message = \"\\t\".join([f\"{k}: {v:.3f}\" for k, v in info.items()])\n",
        "            print(message)\n",
        "            self.save_training_info()\n",
        "            if self.stopper.stop(info[\"val_loss\"]):\n",
        "                break\n",
        "            else:\n",
        "                if self.stopper.testing_patience is False:\n",
        "                    self.save_model()\n",
        ""
      ],
      "metadata": {
        "id": "dw9gnPDCRlJC",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.188137Z",
          "iopub.execute_input": "2021-08-17T05:51:48.188435Z",
          "iopub.status.idle": "2021-08-17T05:51:48.233138Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.188408Z",
          "shell.execute_reply": "2021-08-17T05:51:48.232122Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Qth58Ls6V49Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intializing the datasets and the loaders"
      ],
      "metadata": {
        "id": "UOUlge1TV9eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initer = Initializer()\n",
        "params = initer.initialize(debug=False)"
      ],
      "metadata": {
        "id": "JrA8J4PhTEn-",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:51:48.234351Z",
          "iopub.execute_input": "2021-08-17T05:51:48.23473Z",
          "iopub.status.idle": "2021-08-17T05:52:52.303956Z",
          "shell.execute_reply.started": "2021-08-17T05:51:48.234702Z",
          "shell.execute_reply": "2021-08-17T05:52:52.302956Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intitializing the model and the trainer class"
      ],
      "metadata": {
        "id": "9vlfaXXtWA73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model = PneumoniaNet(input_dim=IMAGE_SIZE).to(device)\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    \"pneumonia_model\",\n",
        "    train_loader=params[\"train_loader\"],\n",
        "    val_loader=params[\"val_loader\"],\n",
        "    test_loader=params[\"test_loader\"],\n",
        "    amp=True,\n",
        ")\n",
        "print(\n",
        "    f\"Total learnable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):.2E}\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Sn0RML4MRAOj",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:52:52.305622Z",
          "iopub.execute_input": "2021-08-17T05:52:52.306071Z",
          "iopub.status.idle": "2021-08-17T05:52:54.982557Z",
          "shell.execute_reply.started": "2021-08-17T05:52:52.306025Z",
          "shell.execute_reply": "2021-08-17T05:52:54.981861Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "cNoVVAzDWFU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "grbo8doRrL7d",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:52:54.98364Z",
          "iopub.execute_input": "2021-08-17T05:52:54.98414Z",
          "iopub.status.idle": "2021-08-17T05:58:17.735553Z",
          "shell.execute_reply.started": "2021-08-17T05:52:54.984089Z",
          "shell.execute_reply": "2021-08-17T05:58:17.731761Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the training history"
      ],
      "metadata": {
        "id": "XgsXqlY_WHiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting loss and f1 score for train and validation\n",
        "fig, axes = plt.subplots(2, 1, figsize=(7,11))\n",
        "\n",
        "training_info = pd.DataFrame(trainer.training_info)\n",
        "\n",
        "training_info.plot(x=\"epoch\",\n",
        "                   y =[\"train_loss\", \"val_loss\"],\n",
        "                   ax=axes[0])\n",
        "axes[0].set_title(\"Loss During Training\")\n",
        "\n",
        "training_info.plot(x=\"epoch\",\n",
        "                   y =[\"train_f1\", \"val_f1\"],\n",
        "                   ax=axes[1])\n",
        "axes[1].set_title(\"F1 Score During Training\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7UejCA8rL_RB",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:58:17.737647Z",
          "iopub.status.idle": "2021-08-17T05:58:17.738338Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting and printing the test results"
      ],
      "metadata": {
        "id": "YZfmcInoWLHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading best model\n",
        "trainer.load_model()\n",
        "trainer.test()"
      ],
      "metadata": {
        "id": "TexLYohMXo8z",
        "execution": {
          "iopub.status.busy": "2021-08-17T05:58:17.739753Z",
          "iopub.status.idle": "2021-08-17T05:58:17.740426Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}